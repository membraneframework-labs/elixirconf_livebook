# Dive Into WebRTC

## Theoretical introduction to WebRTC

### Setup

* Chrome browser [Download](https://www.google.com/chrome/)
* Elixr & Erlang
* Wireshark [Download](https://www.wireshark.org), [Docs](https://www.wireshark.org/docs/wsug_html/)

### Project setup

```bash
mix archive.install hex phx_new
mix phx.new \
  --no-ecto \
  --no-live \
  --no-tailwind \
  --no-mailer \
  --no-gettext \
  --no-dashboard \
  webrtc_playground
```

## getUserMedia

We will start our WebRTC journey by acquiring our local audio and video, then to verify that we acquired it properly we will display our video on our page.  
But let's begin with a brief introduction. For acquiring audio and video we will use [MediaStream API](https://developer.mozilla.org/en-US/docs/Web/API/Media_Capture_and_Streams_API). It is an API related to WebRTC which provides support for streaming audio and video data. This API defines methods that allow you for example: to list all accessible audio and video devices on your computer, to create a MediaStream from Canvas Element and also to get MediaStreams from your current devices. And how you can guess the last one is the functionality that we will use, this function is [MediaDevices.getUserMedia](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia#syntax).
This function returns you a promise of MediaStream object. As for this object, the most important information is it contains audio and video tracks and you can set it as `srcObject` in the video HTML element. After that video element will play this media. For the curios docs of [MediaStream object](https://developer.mozilla.org/en-US/docs/Web/API/MediaStream).

So here is a list of todos (in parentheses will be listed names of files where modifications should be made):

1. Acquire a MediaStream from your local camera (`app.js`)
2. Add video element to page (`index.html.heex`)
3. Insert MediaStream to created video element as it srcObject (`app.js`)

As a result you should see a video of yourself on the main page of application.

<!-- livebook:{"break_markdown":true} -->

### SDP and codecs

<!-- livebook:{"break_markdown":true} -->

Okay, it's a good start, now our next goal will be to create a P2P connection through WebRTC, so we want to see the video of a second peer. To achieve that we will need some more information about the used API.

To create a connection between two peers we will need to create an [RTCPeerConnection object](https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection). It stores all the information about the current connection and we can modify that connection by calling methods on this object. Before sending media through PeerConnection we have to add tracks, which we want to send. We can get these tracks from our MediaStream as it aggregates MediaTracks. Unfortunately, we can't simply add MediaStream to a PeerConnection. Instead, we have to iterate over MediaTracks and add them one by one.

After adding tracks we can create an SDP offer from our PeerConnection. SDP negotiation allows peers to decide what parameters will the media going through connection have. In essence, the offerer sends a list of tracks including their type, direction (send/receive) and possible codecs. The other peer responds with an answer containing that list filtered by his capabilities. It can be seen as an intersection of things supported and accepted by both peers.

So to sum what we want to achieve after this section:

1. Create PeerConnection object (`app.js`)
2. Add tracks to peerConnection (`app.js`)
3. Create SDP offer and print it(`app.js`)

<!-- livebook:{"break_markdown":true} -->

### Further PeerConnection configuration

<!-- livebook:{"break_markdown":true} -->

The next step will be to configure `iceCandidate` handler. ICE candidates are used for connection establishment. Each candidate contains information about what protocol it supports (TCP/UDP/TLS) and what type of candidate it is (host/server reflexive/relay). For now, we can only want to inspect them, so let's log them to the console.

After implementing `iceCandidate` handler we have to change the current local description of our connection. We can do that by calling a proper method on [peerConnection object](https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection). After this step and refreshing browser, you should see logged ICE candidates. Try to answer the following questions:
* how many ice candidates do you see?
* what types do they have?
* what are the differences between candidates?

After that, we will change the initial configuration of PeerConnection. You can do that by passing parameters to its [constructor](https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/RTCPeerConnection). Let's start with setting `bundlePolicy` to `max-bundle`. What did change in comparison to previous candidates?

Next we want to add STUN servers to PeerConnection configuartion, we will use here our Membrane servers:

```javascript
iceServers: [
 {urls: "stun:turn.membraneframework.org:19302"},
 // in case of failure {urls: "stun:stun.l.google.com:19302" }
]
```

What did change after this modification?

Then, we can also add TURN servers to the PeerConnection.

```javascript
iceServers: [
 {
   urls: [
     "turn:turn.membrane.video?transport=udp",
     "turn:turn.membrane.video:3478?transport=tcp",
     "turns:turn.membrane.video:443?transport=tcp"
   ],
   username: "turn",
   credential: "T45B264i89p9"
 }
]
```

And also we could set option `iceTransportPolicy` to `relay`.
How candidates look after this modification?

1. Set iceCandidate handler, which prints ice candidates (`app.js`)
2. Set local description (`app.js`)
3. Modify PeerConnection by adding `max-bundle` parameter (`app.js`)
4. Add iceServers to PeerConnection configuration (`app.js`)
5. Add TURN servers in iceServers configuration (`app.js`)
6. Modify PeerConnection by adding `iceTransportPolicy` parameter (`app.js`)

<!-- livebook:{"break_markdown":true} -->

### Signaling server Part I

<!-- livebook:{"break_markdown":true} -->

After learning a little bit about ice candidates and configuration of PeerConnection we can comment out code responsible for printing out candidates and we can move toward implementing signaling server.

But we should start from that, what signaling server is. 
Signaling server is an intermediary between two peers that want to connect, it also minimizing exposure of potentially private information during establishing connection between these two peers. 
Example information that will go through signaling server are SDP Offer, SDP Answer and ICE candidates.
WebRTC standard doesn't specify transport mechanism for the signaling information, so we will Phoenix Channels for this. 
If someone doesn't know, Phoenix channel is a lightweight wrapper on WebSockets, which allows for creating logic channels inside one connection.

First we have to implement our signaling server. 
We will add two modules: `Room` and `PeerChannel`.

First one is a `GenServer`, which will have to handle two type of messages: `join` and `signal`.
On receiving join it will store pid of this new peer and also check if other peer already exists, if yes it will send `your_message_name` to older peer informing it that it should create SDP offer, otherwise it will do nothing. 
This is only required for deciding which peer will be offerer and the other answerer.
We should also put `Room` under application supervision tree.
On receiving `signal` `Room` will forward this signal message to second peer.

The `PeerChannel` module will be using [`Phoenix.Channel`](https://hexdocs.pm/phoenix/channels.html), it's logic will be also pretty simple message received from the socket will be forwarded to `Room` process and messages from `Room` process will be pushed on socket, additionally in join function `PeerChannel` will send `join` message to `Room`.
Then we have to create instance of class [`Phoenix.Socket`](https://hexdocs.pm/phoenix/js/) and then inside this socket connection we want to create channel with name `"room"`.
After implementing this start application again and you should see `Peer joined` in terminal.

<!-- 1. Comment out code in iceCandidate handler (`app.js`) -->

1. Implement room module (`room.ex`)
2. Implement peerChannel module (`peer_channel.ex`)
3. Create instance of `Phoenix.Socket` and create channel `"room"` in this socket (`app.js`)

<!-- livebook:{"break_markdown":true} -->

### Connecting with second browser

<!-- livebook:{"break_markdown":true} -->

After implementing signaling server and creating channel on frontend we have to implement all event listeners and handlers for message from channel.

We will start from updating `PeerConnection` iceCandidate handler, now it should send candidate through channel to signaling server, instead of printing it.
Next we have to add handlers on messages received from channel.
First we will add handler for `your_message_name` message, after receiving this message this peer should create SDP offer, push it through channel and update local description in Peer connection.
Next we will implement handler for `sdp_offer` message. It should update remote description based on sdp offer, then create sdp answer, forward it through channel and update local description with sdp answer.
The only responsibility for handler of `sdp_answer` message is to update our remote description with this SDP answer.
The `ice_candidate` handler have to add received candidate to Peer connection state.
Last thing to add is an event listener `ontrack` in peer connection, in this listener we have to assign received `MediaStream` to some HTML video element.

1. Modify iceCandidate handler, so it forward candidates to signaling server (`app.js`)
2. Implement handler for `your_message_name` message about creating SDP offer. (`app.js`)
3. Implement handler for `sdp_offer` message (`app.js`)
4. Implement handler for `sdp_answer` message (`app.js`)
5. Implement handler for `ice_candidate` message (`app.js`)
6. Add event listener `ontrack` to `PeerConnection` (`app.js`)

<!-- livebook:{"break_markdown":true} -->

### WebRTC internals in Chrome

<!-- livebook:{"break_markdown":true} -->

You can observe some stats about your connection in [WebRTC internals](chrome://webrtc-internals/).

<!-- livebook:{"break_markdown":true} -->

### Remote test with use of NGROK

<!-- livebook:{"break_markdown":true} -->

The next step in our journey will be connecting two peers from different device.
But there is a small obstacle, page that want to get access to device camera has to have configured SSL cerficate properly and also client must connect through HTTPS. 

So there are two solutions for this: 
- deploy your app through service that will handle SSL for you e.g: [fly.io](https://fly.io/)
- we can handle it with use of [NGROK](https://ngrok.com/), this is recommended way and will be described below

1. Create a free account at NGROK
2. Install [NGROK cli tools](https://ngrok.com/download)
3. Setup your authtoken
4. Create http tunnel to your phoenix app. ([docs](https://ngrok.com/docs/secure-tunnels/tunnels/http-tunnels/))
5. Go to address created by ngrok and test your app.
